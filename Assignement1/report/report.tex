\documentclass[a4paper, 11pt, twoside, openright]{article}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{placeins}
\usepackage{graphicx}
\graphicspath{{../plots/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage{pdflscape}
\usepackage{wrapfig}
\usepackage{amsfonts}

\usepackage{amsmath}
%\DeclareMathOperator*{\argmax}{argmax}
%\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{algpseudocode}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\begin{document}

\section{Theoretical model}
A hypotethical serial algorithm reads $N$ numbers in $T_{read}$ seconds and computes their sum in $T_{comp} \times (N-1)$ seconds. Thus the total time spent is
$$T_{serial}(N) = T_{read} + (N-1)T_{comp}$$
A naive mpi algorithm for the same task, given $P$ processes divided in one master and $P-1$ slaves:
\begin{itemize}
\item the master reads the $N$ numbers in $T_{read}$ seconds
\item the master splits the numbers in $P$ blocks and distributes $P-1$ blocks one to each slave, in $(P-1)T_{comm}$ seconds
\item each process computes the sum of its block of numbers in $\left(\frac{N}{P}-1\right)T_{comp}$ seconds
\item each slave sends its result to the master, which receives them serially in $(P-1)T_{comm}$ seconds
\item the master computes the final sum in $(P-1)T_{comp}$ seconds
\end{itemize}
The time to retrieve the result is then:
$$T_{naive}(N, P) = T_{read} + 2(P-1)T_{comm} + \left(\frac{N}{P}+P-2\right)T_{comp}$$
Notice that for $P=1$ there is no difference with the serial approach.\\
Assuming:
\begin{itemize}
\item $T_{read} = 10^{-4}$ s
\item $T_{comp} = 2 \times 10^{-9}$ s
\item $T_{comm} = 10^{-6}$ s
\end{itemize}
Note: communication time is independent of message size
\begin{figure}[h]
    \includegraphics[width=12cm]{theoretical_naive_t}
    \includegraphics[width=12cm]{theoretical_naive_scalability}
    \caption{Time and scalability of the naive algorithm}
\end{figure}
\FloatBarrier
Scalability is not linear; the algorithm scales only when $P$ is small compared to $N$, then scalability peaks and decreases.\\
Optimal scalability is achieved by $N, P_{opt}$ such that $T_{naive}(N, P_{opt})$ is at its mininum given $N$.\\
$$\frac{\partial T_{naive}(N, P)}{\partial P} = 2T_{comm} + \left(1-\frac{N}{P^{2}}\right)T_{comp}$$
$$2T_{comm} + \left(1-\frac{N}{P_{*}^{2}}\right)T_{comp} = 0$$
$$P_{*} = \sqrt{N\frac{T_{comp}}{2T_{comm} + T_{comp}}}$$
$$P_{opt}(N) = \argmin_{P \in \{ \floor{P_{*}}, \ceil{P_{*}} \}, P \geq 1 }{T_{naive}(N, P)} $$\\
%The derivative with respect to $N$ shows instead that increasing the workload on 
%$$\frac{\partial T_{naive}(N, P)}{\partial N} = \frac{T_{comp}}{P} > 0$$
A different algorithm can be defined, that exploits the system's parallelism by distributing the burden of communications so that they can be performed in parallel, rather than serially by the master.\\
Let the master process have rank $0$ and the slaves have ranks $1$ through $P-1$.\\
Rank $0$ sends to rank $\ceil*{P/2}$, $\floor*{P/2}$ blocks. The problem then reduces to two instances of size $P_{0}=\ceil*{P/2}$ and $P_{1}=\floor*{P/2}$, of which rank $0$ and rank $\ceil*{P/2}$ are the respective masters.\\
The two instances can then proceed in parallel until every process either has received a single block, or has sent all but one, at which point computation of the block's sum can be performed.\\
The number of steps required is $\ceil*{\log_{2}(P)}$ and completes in $\ceil*{\log_{2}(P)}T_{comm}$ seconds.\\
Communication of the result will use the opposite of the previous schema, but the message will contain the partial result and the reveicer will sum it with its own; it will complete in $\ceil*{\log_{2}(P)}(T_{comm} + T_{comp})$ seconds.\\
As data are read like in the naive algorithm and each process computes the sum for a block of the same size, the algorithm completes in
$$T_{enh}(N, P) = T_{read} + \ceil*{\log_{2}(P)} (2T_{comm} + T_{comp}) + \left(\frac{N}{P}-1\right)T_{comp}$$
seconds.\\
\begin{figure}[h]
    \includegraphics[width=12cm]{theoretical_enh_t}
    \includegraphics[width=12cm]{theoretical_enh_scalability}
    \caption{Time and scalability of the enhanced algorithm}
\end{figure}
\FloatBarrier
The graphs show the effect of the ceiling function, with a sharp increase in time when $P$ passes from a power of 2 to the next integer.\\
Having reduced the communication time to an asymptotically sub-linear function of $P$ greatly benefits the scalability, for which the $P_{opt}(N)$ is linear in $N$.\\
Indeed, relaxing the ceiling function:
$$\frac{\partial T_{enh}(N, P)}{\partial P} = \frac{2T_{comm} + T_{comp}}{\ln(2)P} -\frac{N}{P^{2}} T_{comp}$$
$$\frac{2T_{comm} + T_{comp}}{\ln(2)P_{*}} - \frac{N}{P_{*}^{2}} T_{comp} = 0$$
$$P_{*} = N\ln(2)\frac{T_{comp}}{2T_{comm} + T_{comp}}$$
To reconstruct the condition from the first observation:
$$P_{opt}(N) = \argmin_{P \in \{ 2^{\floor{\log_2(P_{*})}}, 2^{\ceil{\log_2(P_{*})}} \}, P \geq 1}{T_{enh}(N, P)} $$\\
Peak scalability is reached for much greater values of $P$ than with the naive algorithm.\\
\end{document}